{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/miniconda3/envs/ai/lib/python3.7/site-packages/pytorch_lightning/core/decorators.py:13: UserWarning: data_loader decorator deprecated in 0.6.1. Will remove 0.8.0\n",
      "  warnings.warn(w)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthEstimator(pl.LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        # Define data paths\n",
    "        self.vkitti_dir = self.vkitti_dir\n",
    "        self.rgb_dirname = \"rgb\"\n",
    "        self.depth_dirname = \"depth\"\n",
    "\n",
    "        self.batch_size = self.__dict__.get(\"batch_size\", 1)\n",
    "        self.learning_rate = self.__dict__.get(\"learning_rate\", 1e-5)\n",
    "\n",
    "        self.stem = torchvision.models.segmentation.fcn_resnet50(progress=True)\n",
    "        self.stem.classifier[4] = torch.nn.Conv2d(\n",
    "            512, 1, kernel_size=(1, 1), stride=(1, 1)\n",
    "        )  # Replace last layer with 1-channel conv\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stem(x)[\"out\"]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return [torch.optim.Adam(self.parameters(), lr=self.learning_rate)]\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        rgb = batch[\"rgb\"]\n",
    "        depth = batch[\"depth\"]\n",
    "        pred = self.forward(rgb)\n",
    "        loss = F.mse_loss(pred, depth)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        rgb = batch[\"rgb\"]\n",
    "        depth = batch[\"depth\"]\n",
    "        pred = self.forward(rgb)\n",
    "        loss = F.mse_loss(pred, depth)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        # avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        # tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "        # return {\"avg_loss\": avg_loss, \"log\": tensorboard_logs}\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, Lambda\n",
    "# Define transforms\n",
    "transforms = {\n",
    "    \"rgb\": Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    \"depth\": Compose([\n",
    "        ToTensor(),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 'rgb' contains 42520 files\n",
      "Subset 'depth' contains 42520 files\n",
      "Found a total of 42520 valid files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rgb': tensor([[[ 1.2728,  1.2728,  1.2728,  ..., -1.4158, -1.3302, -1.0219],\n",
       "          [ 1.2728,  1.2728,  1.2728,  ..., -1.4500, -1.3302, -1.1247],\n",
       "          [ 1.2899,  1.2899,  1.2899,  ..., -1.4329, -1.3987, -1.3130],\n",
       "          ...,\n",
       "          [ 1.9407,  1.9407,  1.9407,  ..., -1.9980, -1.9980, -1.9980],\n",
       "          [ 1.9235,  1.9235,  1.9407,  ..., -1.9980, -1.9980, -1.9980],\n",
       "          [ 1.9235,  1.9235,  1.9407,  ..., -1.9980, -1.9980, -1.9980]],\n",
       " \n",
       "         [[ 1.5532,  1.5532,  1.5532,  ..., -1.3179, -1.2304, -0.9503],\n",
       "          [ 1.5532,  1.5532,  1.5532,  ..., -1.3529, -1.2654, -1.0553],\n",
       "          [ 1.5707,  1.5707,  1.5707,  ..., -1.3704, -1.3354, -1.2304],\n",
       "          ...,\n",
       "          [ 2.1134,  2.1134,  2.1134,  ..., -1.9482, -1.9482, -1.9482],\n",
       "          [ 2.0959,  2.0959,  2.1134,  ..., -1.9482, -1.9482, -1.9482],\n",
       "          [ 2.0959,  2.0959,  2.1134,  ..., -1.9482, -1.9482, -1.9482]],\n",
       " \n",
       "         [[ 1.9428,  1.9428,  1.9428,  ..., -1.0898, -1.0027, -0.7064],\n",
       "          [ 1.9428,  1.9428,  1.9428,  ..., -1.1247, -1.0201, -0.8110],\n",
       "          [ 1.9603,  1.9603,  1.9603,  ..., -1.1247, -1.0898, -1.0376],\n",
       "          ...,\n",
       "          [ 2.3263,  2.3263,  2.2914,  ..., -1.6999, -1.6999, -1.6999],\n",
       "          [ 2.3088,  2.3088,  2.3263,  ..., -1.6999, -1.6999, -1.6999],\n",
       "          [ 2.3088,  2.3088,  2.3263,  ..., -1.6999, -1.6999, -1.6999]]]),\n",
       " 'depth': tensor([[[65535, 65535, 65535,  ...,   995,   993,   990],\n",
       "          [65535, 65535, 65535,  ...,   995,   993,   991],\n",
       "          [65535, 65535, 65535,  ...,   995,   993,   991],\n",
       "          ...,\n",
       "          [  808,   809,   810,  ...,   355,   354,   354],\n",
       "          [  808,   809,   810,  ...,   355,   354,   354],\n",
       "          [  808,   808,   809,  ...,   355,   354,   354]]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "VKITTI_DIR = \"../data/vkitti\"\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset = VkittiImageDataSet(VKITTI_DIR, (\"rgb\", \"depth\"), transforms=transforms)\n",
    "train_dataset[0]\n",
    "# val_dataset = VkittiImageDataSet(VKITTI_DIR, (\"rgb\", \"depth\"), transforms=transforms)\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 'rgb' contains 42520 files\n",
      "Subset 'depth' contains 42520 files\n",
      "Found a total of 42520 valid files.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validation sanity check', layout=Layout(flex='2'), max=5.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d04dd394d04ef6a0e3377f8f1c9c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=1.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 'rgb' contains 42520 files\n",
      "Subset 'depth' contains 42520 files\n",
      "Found a total of 42520 valid files.\n",
      "Subset 'rgb' contains 42520 files\n",
      "Subset 'depth' contains 42520 files\n",
      "Found a total of 42520 valid files.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DepthEstimator(vkitti_dir=VKITTI_DIR, batch_size=2)\n",
    "trainer = pl.Trainer(progress_bar_refresh_rate=1, gpus=[0])\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
